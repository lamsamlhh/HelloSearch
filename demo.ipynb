{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判断爬取下来的url是哪种类型的url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSDN_SEEDS_URL = \"https://blog.csdn.net/\"\n",
    "\n",
    "REGEX_CSDN_USER_MY_URL = \"http[s]*://my\\\\.csdn\\\\.net/\\\\w+\"\n",
    "REGEX_CSDN_USER_BLOG_URL = \"http[s]*://blog\\\\.csdn\\\\.net/\\\\w+\"\n",
    "REGEX_CSDN_BLOG_LIST_URL = \"http[s]*://blog\\\\.csdn\\\\.net/\\\\w+/article/list/\\\\d+\\\\?\"\n",
    "REGEX_CSDN_BLOG_URL = \"http[s]*://blog\\\\.csdn\\\\.net/\\\\w+/article/details/\\\\w+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    urls = f.readlines()\n",
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "USER_MY_URL = []\n",
    "USER_BLOG_URL = []\n",
    "BLOG_LIST_URL = []\n",
    "BLOG_URL = []\n",
    "\n",
    "# 其实也可以简单一点就是一种是博客url，一种是非博客url\n",
    "for url in tqdm(urls):\n",
    "    if re.match(REGEX_CSDN_BLOG_URL, url) != None:\n",
    "        BLOG_URL.append(url)\n",
    "    elif re.match(REGEX_CSDN_BLOG_LIST_URL, url) != None:\n",
    "        BLOG_LIST_URL.append(url)\n",
    "    elif re.match(REGEX_CSDN_USER_MY_URL, url) != None:\n",
    "        USER_MY_URL.append(url)\n",
    "    elif re.match(REGEX_CSDN_USER_BLOG_URL, url) != None:\n",
    "        USER_BLOG_URL.append(url)\n",
    "\n",
    "print(len(USER_MY_URL))\n",
    "print(len(USER_BLOG_URL))\n",
    "print(len(BLOG_LIST_URL))\n",
    "print(len(BLOG_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试GEN的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gne -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.html', 'r', encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = GeneralNewsExtractor().extractor.extract(html, with_body_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gne import GeneralNewsExtractor\n",
    "extractor = GeneralNewsExtractor()\n",
    "result = extractor.extract(html)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试Python Goose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install goose-extractor i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goose\n",
    "\n",
    "g = goose.Goose()\n",
    "article = g.extract(raw_html=html)\n",
    "print(article.title.encode('gbk', 'ignore'))\n",
    "print(article.meta_description.encode('gbk', 'ignore'))\n",
    "print(article.cleaned_text.encode('gbk', 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用xpath提取content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取content\n",
    "'//div[contains(@class, \"content\") and not(contains(@class, \"comment\" or \"\"))]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试html_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSDN \n",
      " ****************************************\n",
      "24\n",
      "11月\n"
     ]
    }
   ],
   "source": [
    "from Engine.html_extractor import MainContent\n",
    "import re\n",
    "import lxml.html\n",
    "from lxml.html import HtmlComment\n",
    "extractor = MainContent()\n",
    "url = \"https://blog.forecho.com/dui-nei-de-2012-nian-zong-jie.html#生活\"\n",
    "with open('./test.html', 'r', encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "title,content = extractor.extract(url, html)\n",
    "print(title, \"\\n\" , \"*\"*40)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判断url是否为内容界面\n",
    "而不是列表界面或者主页又或者用户页等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除http域名这些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 粗略地统计以下url中会含有地关键词,后面可以尝试以下神经网络\n",
    "import jieba\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open('./csdn_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    csdn_urls = f.readlines()\n",
    "with open('./juejin_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    juejin_urls = f.readlines()\n",
    "with open('./sg_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    sg_urls = f.readlines()\n",
    "urls = sg_urls + csdn_urls + juejin_urls\n",
    "print(len(urls))\n",
    "\n",
    "dic_words = {}\n",
    "# nltk不能区分\\\n",
    "# for url in tqdm(urls):\n",
    "#     text = nltk.word_tokenize(url)\n",
    "#     for word in text:\n",
    "#         dic_words[word] = dic_words.get(word, 0) + 1\n",
    "\n",
    "# for url in tqdm(urls):\n",
    "#     text = jieba.cut(url)\n",
    "#     for word in text:\n",
    "#         dic_words[word] = dic_words.get(word, 0) + 1\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    text = re.findall('[a-z]+', url.lower())\n",
    "    for word in text:\n",
    "        dic_words[word] = dic_words.get(word, 0) + 1\n",
    "        \n",
    "df = pd.DataFrame.from_dict(dic_words, orient=\"index\")\n",
    "print(df.sort_values(by=[0],na_position='last'))\n",
    "\n",
    "df.to_csv(\"url_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这类词加分减分\n",
    "# 可以把这些url分词排序然后认为判断加入消极还是积极\n",
    "NEG_WORDS = ['user', 'list', 'authors', 'comment','writers','blogs']\n",
    "POS_WORDS = ['article', 'blog', 'details']\n",
    "\n",
    "# 参考https://help.aliyun.com/document_detail/65096.html  \n",
    "# 还要记住匹配的时候不能区分大小写，同时匹配的时候也仅仅需要匹配url的最后四位就可以了\n",
    "# 这类词一票否决\n",
    "FILE_WORDS = ['gif','png','bmp','jpeg','jpg', 'svg',\n",
    "              'mp3','wma','flv','mp4','wmv','ogg','avi',\n",
    "              'doc','docx','xls','xlsx','ppt','pptx','txt','pdf',\n",
    "              'zip','exe','tat','ico','css','js','swf','apk','m3u8','ts']\n",
    "\n",
    "# 还有就是如果包含很长一串数字的一般都是内容界面\n",
    "# 不应该是各种文件名的后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续数字的匹配\n",
    "import re\n",
    "\n",
    "test_url = \"https://blog.csdn.net/ITF_001?utm_source=feed\"\n",
    "test_str = \"982374589234789\"\n",
    "test_str1 = \"sdfdsfsdfsdf232sdfsdf\"\n",
    "rule1 = \"[0-9]\"*8\n",
    "print(re.match(rule1, test_str, flags=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将爬取下来的相对URL链接转换为绝对链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://fcg.gxepb.gov.cn/hjzf/xzcf/201811/t20181102_46347.html\n",
      "http://fcg.gxepb.gov.cn/hjzf/xzcf/201811/t20181102_46347.html\n"
     ]
    }
   ],
   "source": [
    "from urllib import parse\n",
    " \n",
    "page_url = 'http://fcg.gxepb.gov.cn/ztzl/hjwfbgt/'\n",
    "new_relative_url = 'http://fcg.gxepb.gov.cn/hjzf/xzcf/201811/t20181102_46347.html'\n",
    "new_absolute_url = '../../hjzf/xzcf/201811/t20181102_46347.html'\n",
    "\n",
    "new_full_url1 = parse.urljoin(page_url, new_relative_url)\n",
    "new_full_url2 = parse.urljoin(page_url, new_absolute_url)\n",
    "print(new_full_url1)\n",
    "print(new_full_url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试url_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "https://blog.csdn.net/weixin_36337561/article/details/cat.png\n"
     ]
    }
   ],
   "source": [
    "from Engine.url_parser import is_static_url\n",
    "\n",
    "with open('./csdn_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    csdn_urls = f.readlines()   #https://blog.csdn.net/weixin_36337561/article/details/117780139/cat.png\n",
    "with open('./juejin_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    juejin_urls = f.readlines() #https://blog.csdn.net/weixin_36337561/article/details/117780139\n",
    "with open('./sg_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    sg_urls = f.readlines() #https://blog.csdn.net/weixin_36337561/article/details/117780139\n",
    "urls = sg_urls + csdn_urls + juejin_urls\n",
    "print(len(urls))\n",
    "for i in range(3):\n",
    "    if is_static_url(urls[i]):\n",
    "        print(urls[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试gerapy_auto_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"F:\\\\vscode-workspace\\\\HelloSearch\\\\Engine\")\n",
    "from gerapy_auto_extractor.classifiers.list import is_list\n",
    "from gerapy_auto_extractor.classifiers.detail import is_detail\n",
    "\n",
    "with open(\"./test.html\", 'r', encoding='utf-8') as f:\n",
    "    html= f.read()\n",
    "print(is_detail(html,threshold=0.3))\n",
    "print(is_list(html,threshold=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/My_app/code/咻Search/Engine\")\n",
    "from gerapy_auto_extractor.extractors.content import extract_content\n",
    "from gerapy_auto_extractor.extractors.datetime import extract_datetime\n",
    "from gerapy_auto_extractor.extractors.list import extract_list\n",
    "from gerapy_auto_extractor.extractors.title import extract_title\n",
    "\n",
    "with open(\"./test.html\", 'r', encoding='utf-8') as f:\n",
    "    html= f.read()\n",
    "    \n",
    "# print(extract_title(html))\n",
    "# print(extract_list(html))\n",
    "print(extract_datetime(html))\n",
    "\n",
    "# 这个效果极其垃圾\n",
    "# content_html = extract_content(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试查出mysql数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from config import MYSQL_HOST, MYSQL_DBNAME, MYSQL_USER, MYSQL_PASSWORD\n",
    "\n",
    "# 连接数据库\n",
    "# 加上charset='utf8'，避免 'latin-1' encoding 报错等问题\n",
    "conn = pymysql.connect(host=MYSQL_HOST, user=MYSQL_USER, passwd=MYSQL_PASSWORD, \n",
    "                       db=MYSQL_DBNAME, charset='utf8')\n",
    "\n",
    "# 创建cursor\n",
    "cursor_blogs = conn.cursor()\n",
    "cursor_list = conn.cursor()\n",
    "sql_blogs = 'SELECT page_url, urls FROM search_blogs;'\n",
    "sql_list = 'SELECT page_url, urls FROM search_blogs;'\n",
    "\n",
    "# 执行sql语句\n",
    "cursor_blogs.execute(sql_blogs)\n",
    "cursor_list.execute(sql_list)\n",
    "# 获取数据库列表信息\n",
    "col_blogs = cursor_blogs.description\n",
    "col_list = cursor_list.description\n",
    "\n",
    "# 获取全部查询信息\n",
    "re_blogs = cursor_blogs.fetchall()\n",
    "re_list = cursor_list.fetchall()\n",
    "# 获取一行信息\n",
    "# re = cursor.fetchone()\n",
    "\n",
    "# 获取的信息默认为tuple类型，将columns转换成DataFrame类型\n",
    "columns_blogs = pd.DataFrame(list(col_blogs))\n",
    "# 将数据转换成DataFrame类型，并匹配columns\n",
    "df_blogs = pd.DataFrame(list(re_blogs), columns=columns_blogs[0])\n",
    "\n",
    "columns_list = pd.DataFrame(list(col_blogs))\n",
    "# 将数据转换成DataFrame类型，并匹配columns\n",
    "df_list = pd.DataFrame(list(re_blogs), columns=columns_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "blogs_index = [url[0] for  url in re_blogs]\n",
    "list_index = [ast.literal_eval(url[1]) for  url in re_blogs]\n",
    "list_index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23384/2126155664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0murls2G\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23384/2126155664.py\u001b[0m in \u001b[0;36murls2G\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mp_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mp_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mp_index\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pymysql\n",
    "import ast\n",
    "import sys\n",
    "sys.path.append(\"F:\\\\vscode-workspace\\\\HelloSearch\\\\Engine\")\n",
    "sys.path.append(\"F:\\\\vscode-workspace\\\\HelloSearch\")\n",
    "from config import MYSQL_HOST, MYSQL_DBNAME, MYSQL_USER, MYSQL_PASSWORD\n",
    "def urls2G():\n",
    "    '''\n",
    "    将数据库中urls的关系转化为图\n",
    "    '''\n",
    "    # 连接数据库\n",
    "    # 加上charset='utf8'，避免 'latin-1' encoding 报错等问题\n",
    "    conn = pymysql.connect(host=MYSQL_HOST, user=MYSQL_USER, passwd=MYSQL_PASSWORD, \n",
    "                        db=MYSQL_DBNAME, charset='utf8')\n",
    "    # 创建cursor\n",
    "    cursor_blogs = conn.cursor()\n",
    "    cursor_list = conn.cursor()\n",
    "    sql_blogs = 'SELECT page_url, urls FROM search_blogs;'\n",
    "    sql_list = 'SELECT page_url, urls FROM search_blogs;'\n",
    "    # 执行sql语句\n",
    "    cursor_blogs.execute(sql_blogs)\n",
    "    cursor_list.execute(sql_list)\n",
    "    # 获取全部查询信息\n",
    "    re_blogs = cursor_blogs.fetchall()\n",
    "    re_list = cursor_list.fetchall()\n",
    "    \n",
    "    # 将获取的元组信息转换为图\n",
    "    blogs_index = [url[0] for  url in re_blogs]\n",
    "    blogs_point = [ast.literal_eval(url[1]) for  url in re_blogs]\n",
    "    \n",
    "    list_index = [url[0] for  url in re_list]\n",
    "    list_point = [ast.literal_eval(url[1]) for  url in re_list]\n",
    "    indexs = blogs_index + list_index\n",
    "    points = blogs_point + list_point\n",
    "    G = np.zeros((len(indexs), len(indexs)))\n",
    "    for i, index in enumerate(indexs):\n",
    "        # 依次判断包含的url是是否在爬取过的列表中，有些广告之类的链接页会包含，但没爬取\n",
    "        for p_url in points[i]:\n",
    "            try:\n",
    "                p_index = indexs.index(p_url)\n",
    "            except:\n",
    "                p_index = -1\n",
    "            if p_index != -1:\n",
    "                G[i][p_index] = 1\n",
    "                \n",
    "    return G\n",
    "urls2G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47e86d731e077963188d400b641a1f5cee6401b89b8a1175acb1a082248e2517"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
