{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判断爬取下来的url是哪种类型的url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSDN_SEEDS_URL = \"https://blog.csdn.net/\"\n",
    "\n",
    "REGEX_CSDN_USER_MY_URL = \"http[s]*://my\\\\.csdn\\\\.net/\\\\w+\"\n",
    "REGEX_CSDN_USER_BLOG_URL = \"http[s]*://blog\\\\.csdn\\\\.net/\\\\w+\"\n",
    "REGEX_CSDN_BLOG_LIST_URL = \"http[s]*://blog\\\\.csdn\\\\.net/\\\\w+/article/list/\\\\d+\\\\?\"\n",
    "REGEX_CSDN_BLOG_URL = \"http[s]*://blog\\\\.csdn\\\\.net/\\\\w+/article/details/\\\\w+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://blog.csdn.net/nav/python\\n',\n",
       " 'https://g.csdnimg.cn/static/logo/favicon32.ico\\n',\n",
       " '//csdnimg.cn/public/common/toolbar/content_toolbar_css/content_toolbar.css\\n',\n",
       " '//csdnimg.cn/public/common/libs/bootstrap/css/bootstrap.min.css\\n',\n",
       " '//csdnimg.cn/public/static/css/avatar.css\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    urls = f.readlines()\n",
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1459/1459 [00:00<00:00, 97287.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "665\n",
      "0\n",
      "707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "USER_MY_URL = []\n",
    "USER_BLOG_URL = []\n",
    "BLOG_LIST_URL = []\n",
    "BLOG_URL = []\n",
    "\n",
    "# 其实也可以简单一点就是一种是博客url，一种是非博客url\n",
    "for url in tqdm(urls):\n",
    "    if re.match(REGEX_CSDN_BLOG_URL, url) != None:\n",
    "        BLOG_URL.append(url)\n",
    "    elif re.match(REGEX_CSDN_BLOG_LIST_URL, url) != None:\n",
    "        BLOG_LIST_URL.append(url)\n",
    "    elif re.match(REGEX_CSDN_USER_MY_URL, url) != None:\n",
    "        USER_MY_URL.append(url)\n",
    "    elif re.match(REGEX_CSDN_USER_BLOG_URL, url) != None:\n",
    "        USER_BLOG_URL.append(url)\n",
    "\n",
    "print(len(USER_MY_URL))\n",
    "print(len(USER_BLOG_URL))\n",
    "print(len(BLOG_LIST_URL))\n",
    "print(len(BLOG_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试GEN的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gne -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.html', 'r', encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = GeneralNewsExtractor().extractor.extract(html, with_body_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'python scrapy 代理中间件,爬虫必掌握的内容之一', 'author': '', 'publish_time': '2021-11-13 08:36:24', 'content': '基于\\nPython\\nScrapy\\n实现的蜂鸟数据采集\\n爬虫\\n系统 含\\n代理\\n、日志处理和全部源代码等import\\nscrapy\\nfrom fengniao.items import FengniaoItemfrom\\nscrapy\\n.spidermiddlewares.httperror import HttpErrorfrom twisted.internet.error import TimeoutError, TCPTimedOutError, DNSLookupError, ConnectionRefusedErrorclass FengniaoclawerSpider(\\nscrapy\\n.Spider):\\tname = \\'fengniaoClawer\\'\\tallowed_domains = [\\'fengniao.com\\']\\t#\\n爬虫\\n自定义设置,会覆盖 settings.\\npy\\n文件中的设置\\tcustom_settings = {\\t\\t\\'LOG_LEVEL\\': \\'DEBUG\\', # 定义log等级\\t\\t\\'DOWNLOAD_DELAY\\': 0, # 下载延时\\t\\t\\'COOKIES_ENABLED\\': False, # enabled by default\\t\\t\\'DEFAULT_REQUEST_HEADERS\\': {\\t\\t\\t# \\'Host\\': \\'www.fengniao.com\\',\\t\\t\\t\\'Referer\\': \\'https://www.fengniao.com\\',\\t\\t},\\t\\t# 管道文件,优先级按照由小到大依次进入\\t\\t\\'ITEM_PIPELINES\\': {\\t\\t\\t\\'fengniao.pipelines.ImagePipeline\\': 100,\\t\\t\\t\\'fengniao.pipelines.FengniaoPipeline\\': 300,\\t\\t},\\t\\t# 关于下载图片部分\\t\\t\\'IMAGES_STORE\\': \\'fengniaoPhoto\\', # 没有则新建\\t\\t\\'IMAGES_EXPIRES\\': 90, # 图片有效期,已经存在的图片在这个时间段内不会再下载\\t\\t\\'IMAGES_MIN_HEIGHT\\': 100, # 图片最小尺寸(高度),低于这个高度的图片不会下载\\t\\t\\'IMAGES_MIN_WIDTH\\': 100, # 图片最小尺寸(宽度),低于这个宽度的图片不会下载\\t\\t# 下载\\n中间件\\n,优先级按照由小到大依次进入\\t\\t\\'DOWNLOADER_MIDDLEWARES\\': {\\t\\t\\t\\'fengniao.middlewares.ProxiesMiddleware\\': 400,\\t\\t\\t\\'fengniao.middlewares.HeadersMiddleware\\': 543,\\t\\t\\t\\'\\nscrapy\\n.downloadermiddlewares.use\\nra\\ngent.Use\\nrA\\ngentMiddleware\\': None,\\t\\t},\\t\\t\\'DEPTH_PRIORITY\\': 1, # BFS,是以starts_url为准,局部BFS,受CONCURRENT_REQUESTS影响\\t\\t\\'\\nSC\\nHEDULER_DISK_QUEUE\\': \\'\\nscrapy\\n.squeues.PickleFifoDiskQueue\\',\\t\\t\\'\\nSC\\nHEDULER_MEMORY_QUEUE\\': \\'\\nscrapy\\n.squeues.FifoMemoryQueue\\',\\t\\t\\'REDIRECT_PRIORITY_ADJUST\\': 2, # Default: +2\\t\\t\\'RETRY_PRIORITY_ADJUST\\': -1, # Default: -1\\t\\t\\'RETRY_TIMES\\': 8, # 重试次数\\t\\t# Default: 2, can also be specified per-request using max_retry_times attribute of Request.meta\\t\\t\\'DOWNLOAD_TIMEOUT\\': 30,\\t\\t# This timeout can be set per spider using download_timeout spider attribute and per-request using download_timeout Request.meta key\\t\\t# \\'DUPEFILTER_CLASS\\': \"\\nscrapy\\n_redis.dupefilter.RFPDupeFilter\",\\t\\t# \\'\\nSC\\nHEDULER\\': \"\\nscrapy\\n_redis.\\nsc\\nheduler.\\nSc\\nheduler\",\\t\\t# \\'\\nSC\\nHEDULER_PERSIST\\': False, # Don\\'t cleanup red', 'images': []}\n"
     ]
    }
   ],
   "source": [
    "from gne import GeneralNewsExtractor\n",
    "extractor = GeneralNewsExtractor()\n",
    "result = extractor.extract(html)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'基于\\nPython\\nScrapy\\n实现的蜂鸟数据采集\\n爬虫\\n系统 含\\n代理\\n、日志处理和全部源代码等import\\nscrapy\\nfrom fengniao.items import FengniaoItemfrom\\nscrapy\\n.spidermiddlewares.httperror import HttpErrorfrom twisted.internet.error import TimeoutError, TCPTimedOutError, DNSLookupError, ConnectionRefusedErrorclass FengniaoclawerSpider(\\nscrapy\\n.Spider):\\tname = \\'fengniaoClawer\\'\\tallowed_domains = [\\'fengniao.com\\']\\t#\\n爬虫\\n自定义设置,会覆盖 settings.\\npy\\n文件中的设置\\tcustom_settings = {\\t\\t\\'LOG_LEVEL\\': \\'DEBUG\\', # 定义log等级\\t\\t\\'DOWNLOAD_DELAY\\': 0, # 下载延时\\t\\t\\'COOKIES_ENABLED\\': False, # enabled by default\\t\\t\\'DEFAULT_REQUEST_HEADERS\\': {\\t\\t\\t# \\'Host\\': \\'www.fengniao.com\\',\\t\\t\\t\\'Referer\\': \\'https://www.fengniao.com\\',\\t\\t},\\t\\t# 管道文件,优先级按照由小到大依次进入\\t\\t\\'ITEM_PIPELINES\\': {\\t\\t\\t\\'fengniao.pipelines.ImagePipeline\\': 100,\\t\\t\\t\\'fengniao.pipelines.FengniaoPipeline\\': 300,\\t\\t},\\t\\t# 关于下载图片部分\\t\\t\\'IMAGES_STORE\\': \\'fengniaoPhoto\\', # 没有则新建\\t\\t\\'IMAGES_EXPIRES\\': 90, # 图片有效期,已经存在的图片在这个时间段内不会再下载\\t\\t\\'IMAGES_MIN_HEIGHT\\': 100, # 图片最小尺寸(高度),低于这个高度的图片不会下载\\t\\t\\'IMAGES_MIN_WIDTH\\': 100, # 图片最小尺寸(宽度),低于这个宽度的图片不会下载\\t\\t# 下载\\n中间件\\n,优先级按照由小到大依次进入\\t\\t\\'DOWNLOADER_MIDDLEWARES\\': {\\t\\t\\t\\'fengniao.middlewares.ProxiesMiddleware\\': 400,\\t\\t\\t\\'fengniao.middlewares.HeadersMiddleware\\': 543,\\t\\t\\t\\'\\nscrapy\\n.downloadermiddlewares.use\\nra\\ngent.Use\\nrA\\ngentMiddleware\\': None,\\t\\t},\\t\\t\\'DEPTH_PRIORITY\\': 1, # BFS,是以starts_url为准,局部BFS,受CONCURRENT_REQUESTS影响\\t\\t\\'\\nSC\\nHEDULER_DISK_QUEUE\\': \\'\\nscrapy\\n.squeues.PickleFifoDiskQueue\\',\\t\\t\\'\\nSC\\nHEDULER_MEMORY_QUEUE\\': \\'\\nscrapy\\n.squeues.FifoMemoryQueue\\',\\t\\t\\'REDIRECT_PRIORITY_ADJUST\\': 2, # Default: +2\\t\\t\\'RETRY_PRIORITY_ADJUST\\': -1, # Default: -1\\t\\t\\'RETRY_TIMES\\': 8, # 重试次数\\t\\t# Default: 2, can also be specified per-request using max_retry_times attribute of Request.meta\\t\\t\\'DOWNLOAD_TIMEOUT\\': 30,\\t\\t# This timeout can be set per spider using download_timeout spider attribute and per-request using download_timeout Request.meta key\\t\\t# \\'DUPEFILTER_CLASS\\': \"\\nscrapy\\n_redis.dupefilter.RFPDupeFilter\",\\t\\t# \\'\\nSC\\nHEDULER\\': \"\\nscrapy\\n_redis.\\nsc\\nheduler.\\nSc\\nheduler\",\\t\\t# \\'\\nSC\\nHEDULER_PERSIST\\': False, # Don\\'t cleanup red'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试Python Goose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/simple (17.6 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Cannot unpack file C:\\Users\\Justin3go\\AppData\\Local\\Temp\\pip-unpack-t8w7a3my\\simple.html (downloaded from C:\\Users\\Justin3go\\AppData\\Local\\Temp\\pip-req-build-xhzn4cnj, content-type: text/html); cannot detect archive format\n",
      "ERROR: Cannot determine archive format of C:\\Users\\Justin3go\\AppData\\Local\\Temp\\pip-req-build-xhzn4cnj\n"
     ]
    }
   ],
   "source": [
    "! pip install goose-extractor i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'goose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JUSTIN~1\\AppData\\Local\\Temp/ipykernel_9204/4131557277.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoose\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGoose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_html\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gbk'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'goose'"
     ]
    }
   ],
   "source": [
    "import goose\n",
    "\n",
    "g = goose.Goose()\n",
    "article = g.extract(raw_html=html)\n",
    "print(article.title.encode('gbk', 'ignore'))\n",
    "print(article.meta_description.encode('gbk', 'ignore'))\n",
    "print(article.cleaned_text.encode('gbk', 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用xpath提取content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取content\n",
    "'//div[contains(@class, \"content\") and not(contains(@class, \"comment\" or \"\"))]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试html_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python scrapy 代理中间件，爬虫必掌握的内容之一_梦想橡皮擦，专栏100例写作模式先行者 \n",
      " ****************************************\n",
      "本篇博客为大家说明一下 scrapy 中代理相关知识点。\n",
      "代理的使用场景\n",
      "编写爬虫代码的程序员，永远绕不开就是使用代理，在编码过程中，你会碰到如下情形：\n",
      "网络不好，需要代理；目标站点国内访问不了，需要代理；网站封杀了你的 IP，需要代理。\n",
      "使用 HttpProxyMiddleware 中间件\n",
      "本次的测试站点依旧使用 http://httpbin.org/，通过访问 http://httpbin.org/ip 可以获取当前请求的 IP 地址。\n",
      "HttpProxyMiddleware 中间件默认是开启的，可以查看其源码重点为\n",
      "process_request() 方法。\n",
      "修改代理的方式非常简单，只需要在 Requests 请求创建的时候，增加 meta 参数即可。\n",
      "import scrapy\n",
      "class PtSpider(scrapy.Spider):\n",
      "name = 'pt'\n",
      "allowed_domains = ['httpbin.org']\n",
      "start_urls = ['http://httpbin.org/ip']\n",
      "def start_requests(self):\n",
      "yield scrapy.Request(url=self.start_urls[0], meta={'proxy': 'http://202.5.116.49:8080'})\n",
      "def parse(self, response):\n",
      "print(response.text)\n",
      "接下来通过获取一下 https://www.kuaidaili.com/free/ 网站的代理 IP，并测试其代理是否可用。\n",
      "import scrapy\n",
      "class PtSpider(scrapy.Spider):\n",
      "name = 'pt'\n",
      "allowed_domains = ['httpbin.org', 'kuaidaili.com']\n",
      "start_urls = ['https://www.kuaidaili.com/free/']\n",
      "def parse(self, response):\n",
      "IP = response.xpath('//td[@data-title=\"IP\"]/text()').getall()\n",
      "PORT = response.xpath('//td[@data-title=\"PORT\"]/text()').getall()\n",
      "url = 'http://httpbin.org/ip'\n",
      "for ip, port in zip(IP, PORT):\n",
      "proxy = f\"http://{ip}:{port}\"\n",
      "meta = {\n",
      "'proxy': proxy,\n",
      "'dont_retry': True,\n",
      "'download_timeout': 10,\n",
      "}\n",
      "yield scrapy.Request(url=url, callback=self.check_proxy, meta=meta, dont_filter=True)\n",
      "def check_proxy(self, response):\n",
      "print(response.text)\n",
      "接下来将可用的代理 IP 保存到 JSON 文件中。\n",
      "import scrapy\n",
      "class PtSpider(scrapy.Spider):\n",
      "name = 'pt'\n",
      "allowed_domains = ['httpbin.org', 'kuaidaili.com']\n",
      "start_urls = ['https://www.kuaidaili.com/free/']\n",
      "def parse(self, response):\n",
      "IP = response.xpath('//td[@data-title=\"IP\"]/text()').getall()\n",
      "PORT = response.xpath('//td[@data-title=\"PORT\"]/text()').getall()\n",
      "url = 'http://httpbin.org/ip'\n",
      "for ip, port in zip(IP, PORT):\n",
      "proxy = f\"http://{ip}:{port}\"\n",
      "meta = {\n",
      "'proxy': proxy,\n",
      "'dont_retry': True,\n",
      "'download_timeout': 10,\n",
      "'_proxy': proxy\n",
      "}\n",
      "yield scrapy.Request(url=url, callback=self.check_proxy, meta=meta, dont_filter=True)\n",
      "def check_proxy(self, response):\n",
      "proxy_ip = response.json()['origin']\n",
      "if proxy_ip is not None:\n",
      "yield {\n",
      "'proxy': response.meta['_proxy']\n",
      "}\n",
      "同时修改 start_requests 方法，获取 10 页代理。\n",
      "class PtSpider(scrapy.Spider):\n",
      "name = 'pt'\n",
      "allowed_domains = ['httpbin.org', 'kuaidaili.com']\n",
      "url_format = 'https://www.kuaidaili.com/free/inha/{}/'\n",
      "def start_requests(self):\n",
      "for page in range(1, 11):\n",
      "yield scrapy.Request(url=self.url_format.format(page))\n",
      "实现一个自定义的代理中间件也比较容易，有两种办法，第一种继承 HttpProxyMiddleware，编写如下代码：\n",
      "from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\n",
      "from collections import defaultdict\n",
      "import random\n",
      "class RandomProxyMiddleware(HttpProxyMiddleware):\n",
      "def __init__(self, auth_encoding='latin-1'):\n",
      "self.auth_encoding = auth_encoding\n",
      "self.proxies = defaultdict(list)\n",
      "with open('./proxy.csv') as f:\n",
      "proxy_list = f.readlines()\n",
      "for proxy in proxy_list:\n",
      "scheme = 'http'\n",
      "url = proxy.strip()\n",
      "self.proxies[scheme].append(self._get_proxy(url, scheme))\n",
      "def _set_proxy(self, request, scheme):\n",
      "creds, proxy = random.choice(self.proxies[scheme])\n",
      "request.meta['proxy'] = proxy\n",
      "if creds:\n",
      "request.headers['Proxy-Authorization'] = b'Basic ' + creds\n",
      "代码核心重写了 __init__ 构造方法，并重写了 _set_proxy 方法，在其中实现了随机代理获取。\n",
      "同步修改\n",
      "settings.py 文件中的代码。\n",
      "DOWNLOADER_MIDDLEWARES = {\n",
      "'proxy_text.middlewares.RandomProxyMiddleware': 543,\n",
      "}\n",
      "创建一个新的代理中间件类\n",
      "class NRandomProxyMiddleware(object):\n",
      "def __init__(self, settings):\n",
      "# 从settings中读取代理配置 PROXIES\n",
      "self.proxies = settings.getlist(\"PROXIES\")\n",
      "def process_request(self, request, spider):\n",
      "request.meta[\"proxy\"] = random.choice(self.proxies)\n",
      "@classmethod\n",
      "def from_crawler(cls, crawler):\n",
      "if not crawler.settings.getbool(\"HTTPPROXY_ENABLED\"):\n",
      "raise NotConfigured\n",
      "return cls(crawler.settings)\n",
      "可以看到该类从 settings.py 文件中的 PROXIES 读取配置，所以修改对应配置如下所示：\n",
      "DOWNLOADER_MIDDLEWARES = {\n",
      "'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': None,\n",
      "'proxy_text.middlewares.NRandomProxyMiddleware': 543,\n",
      "}\n",
      "# 代码是前文代码采集的结果\n",
      "PROXIES = ['http://140.249.48.241:6969',\n",
      "'http://47.96.16.149:80',\n",
      "'http://140.249.48.241:6969',\n",
      "'http://47.100.14.22:9006',\n",
      "'http://47.100.14.22:9006']\n",
      "如果你想测试爬虫，可编写一个随机返回请求代理的函数，将其用到任意爬虫代码之上，完成本博客任务。\n",
      "收藏时间\n",
      "本期博客收藏过 400，立刻更新下一篇\n",
      "今天是持续写作的第 261 / 200 天。\n",
      "可以\n",
      "关注我，点赞我、评论我、收藏我啦。\n",
      "更多精彩\n",
      "👇👇👇扫码加入【78技术人】~ Python 事业部👇👇👇，源码也在这\n"
     ]
    }
   ],
   "source": [
    "from Engine.html_extractor import MainContent\n",
    "extractor = MainContent()\n",
    "url = \"https://blog.csdn.net/hihell/article/details/121012464\"\n",
    "with open('./test.html', 'r', encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "title,content = extractor.extract(url, html)\n",
    "print(title, \"\\n\" , \"*\"*40)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判断url是否为内容界面\n",
    "而不是列表界面或者主页又或者用户页等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除http域名这些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8575/8575 [00:00<00:00, 120781.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0\n",
      "taro         1\n",
      "mac          1\n",
      "echarts      1\n",
      "webstorm     1\n",
      "markdown     1\n",
      "...        ...\n",
      "a         1998\n",
      "blog      2059\n",
      "net       2146\n",
      "csdn      2175\n",
      "https     2258\n",
      "\n",
      "[898 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 粗略地统计以下url中会含有地关键词,后面可以尝试以下神经网络\n",
    "import jieba\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open('./csdn_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    csdn_urls = f.readlines()\n",
    "with open('./juejin_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    juejin_urls = f.readlines()\n",
    "with open('./sg_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    sg_urls = f.readlines()\n",
    "urls = sg_urls + csdn_urls + juejin_urls\n",
    "print(len(urls))\n",
    "\n",
    "dic_words = {}\n",
    "# nltk不能区分\\\n",
    "# for url in tqdm(urls):\n",
    "#     text = nltk.word_tokenize(url)\n",
    "#     for word in text:\n",
    "#         dic_words[word] = dic_words.get(word, 0) + 1\n",
    "\n",
    "# for url in tqdm(urls):\n",
    "#     text = jieba.cut(url)\n",
    "#     for word in text:\n",
    "#         dic_words[word] = dic_words.get(word, 0) + 1\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    text = re.findall('[a-z]+', url.lower())\n",
    "    for word in text:\n",
    "        dic_words[word] = dic_words.get(word, 0) + 1\n",
    "        \n",
    "df = pd.DataFrame.from_dict(dic_words, orient=\"index\")\n",
    "print(df.sort_values(by=[0],na_position='last'))\n",
    "\n",
    "df.to_csv(\"url_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这类词加分减分\n",
    "# 可以把这些url分词排序然后认为判断加入消极还是积极\n",
    "NEG_WORDS = ['user', 'list', 'authors', 'comment','writers','blogs']\n",
    "POS_WORDS = ['article', 'blog', 'details']\n",
    "\n",
    "# 参考https://help.aliyun.com/document_detail/65096.html  \n",
    "# 还要记住匹配的时候不能区分大小写，同时匹配的时候也仅仅需要匹配url的最后四位就可以了\n",
    "# 这类词一票否决\n",
    "FILE_WORDS = ['gif','png','bmp','jpeg','jpg', 'svg',\n",
    "              'mp3','wma','flv','mp4','wmv','ogg','avi',\n",
    "              'doc','docx','xls','xlsx','ppt','pptx','txt','pdf',\n",
    "              'zip','exe','tat','ico','css','js','swf','apk','m3u8','ts']\n",
    "\n",
    "# 还有就是如果包含很长一串数字的一般都是内容界面\n",
    "# 不应该是各种文件名的后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 8), match='98237458'>\n"
     ]
    }
   ],
   "source": [
    "# 连续数字的匹配\n",
    "import re\n",
    "\n",
    "test_url = \"https://blog.csdn.net/ITF_001?utm_source=feed\"\n",
    "test_str = \"982374589234789\"\n",
    "test_str1 = \"sdfdsfsdfsdf232sdfsdf\"\n",
    "rule1 = \"[0-9]\"*8\n",
    "print(re.match(rule1, test_str, flags=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将爬取下来的相对URL链接转换为绝对链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://fcg.gxepb.gov.cn/hjzf/xzcf/201811/t20181102_46347.html\n",
      "http://fcg.gxepb.gov.cn/hjzf/xzcf/201811/t20181102_46347.html\n"
     ]
    }
   ],
   "source": [
    "from urllib import parse\n",
    " \n",
    "page_url = 'http://fcg.gxepb.gov.cn/ztzl/hjwfbgt/'\n",
    "new_relative_url = 'http://fcg.gxepb.gov.cn/hjzf/xzcf/201811/t20181102_46347.html'\n",
    "new_absolute_url = '../../hjzf/xzcf/201811/t20181102_46347.html'\n",
    "\n",
    "new_full_url1 = parse.urljoin(page_url, new_relative_url)\n",
    "new_full_url2 = parse.urljoin(page_url, new_absolute_url)\n",
    "print(new_full_url1)\n",
    "print(new_full_url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试url_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8575\n",
      "https://cdn.segmentfault.com/r-35ff81af/touch-icon.png\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/favicon.ico\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/umi.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index~p__Video__Add.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index~p__Video__Add.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/p__Portal.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/p__Portal.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/touch-icon.png\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/favicon.ico\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/umi.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index~p__Video__Add.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index~p__Video__Add.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/layouts__index.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/vendors~p__Blogs.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/vendors~p__Blogs.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/p__Blogs.chunk.css\n",
      "\n",
      "https://cdn.segmentfault.com/r-35ff81af/p__Blogs.chunk.css\n",
      "\n",
      "https://g.csdnimg.cn/static/logo/favicon32.ico\n",
      "\n",
      "//csdnimg.cn/public/common/toolbar/content_toolbar_css/content_toolbar.css\n",
      "\n",
      "//csdnimg.cn/public/common/libs/bootstrap/css/bootstrap.min.css\n",
      "\n",
      "//csdnimg.cn/public/static/css/avatar.css\n",
      "\n",
      "//g.csdnimg.cn/nav-second/1.0.3/css/nav-second.css\n",
      "\n",
      "https://g.csdnimg.cn/common/csdn-toolbar/csdn-toolbar-default.css\n",
      "\n",
      "https://g.csdnimg.cn/user-tooltip/2.5/user-tooltip.css\n",
      "\n",
      "https://g.csdnimg.cn/side-toolbar/3.0/side-toolbar.css\n",
      "\n",
      "//g.csdnimg.cn/common/csdn-footer/csdn-footer.css\n",
      "\n",
      "https://csdnimg.cn/cdn/content-toolbar/csdn-ICP.png\n",
      "\n",
      "https://csdnimg.cn/release/live_fe/culture_license.png\n",
      "\n",
      "https://img-home.csdnimg.cn/images/20210414021142.jpg\n",
      "\n",
      "https://img-home.csdnimg.cn/images/20210414021151.jpg\n",
      "\n",
      "https://g.csdnimg.cn/static/logo/favicon32.ico\n",
      "\n",
      "//csdnimg.cn/public/common/toolbar/content_toolbar_css/content_toolbar.css\n",
      "\n",
      "//csdnimg.cn/public/common/libs/bootstrap/css/bootstrap.min.css\n",
      "\n",
      "//csdnimg.cn/public/static/css/avatar.css\n",
      "\n",
      "//g.csdnimg.cn/nav-second/1.0.3/css/nav-second.css\n",
      "\n",
      "https://g.csdnimg.cn/common/csdn-toolbar/csdn-toolbar-default.css\n",
      "\n",
      "https://g.csdnimg.cn/user-tooltip/2.5/user-tooltip.css\n",
      "\n",
      "https://g.csdnimg.cn/side-toolbar/3.0/side-toolbar.css\n",
      "\n",
      "//g.csdnimg.cn/common/csdn-footer/csdn-footer.css\n",
      "\n",
      "https://csdnimg.cn/cdn/content-toolbar/csdn-ICP.png\n",
      "\n",
      "https://csdnimg.cn/release/live_fe/culture_license.png\n",
      "\n",
      "https://img-home.csdnimg.cn/images/20210414021142.jpg\n",
      "\n",
      "https://img-home.csdnimg.cn/images/20210414021151.jpg\n",
      "\n",
      "https://g.csdnimg.cn/static/logo/favicon32.ico\n",
      "\n",
      "//csdnimg.cn/public/common/toolbar/content_toolbar_css/content_toolbar.css\n",
      "\n",
      "//csdnimg.cn/public/common/libs/bootstrap/css/bootstrap.min.css\n",
      "\n",
      "//csdnimg.cn/public/static/css/avatar.css\n",
      "\n",
      "//g.csdnimg.cn/nav-second/1.0.3/css/nav-second.css\n",
      "\n",
      "https://g.csdnimg.cn/common/csdn-toolbar/csdn-toolbar-default.css\n",
      "\n",
      "https://g.csdnimg.cn/user-tooltip/2.5/user-tooltip.css\n",
      "\n",
      "https://g.csdnimg.cn/side-toolbar/3.0/side-toolbar.css\n",
      "\n",
      "//g.csdnimg.cn/common/csdn-footer/csdn-footer.css\n",
      "\n",
      "https://csdnimg.cn/cdn/content-toolbar/csdn-ICP.png\n",
      "\n",
      "https://csdnimg.cn/release/live_fe/culture_license.png\n",
      "\n",
      "https://img-home.csdnimg.cn/images/20210414021142.jpg\n",
      "\n",
      "https://img-home.csdnimg.cn/images/20210414021151.jpg\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/apple-touch-icon.png\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/favicon-32x32.png\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/favicon-16x16.png\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/safari-pinned-tab.svg\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/ionicons/css/ionicons.min.css\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/iconfont/index.css\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/ionicons/iconfont.css\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/bytedesign.min.css\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e506a9f.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/176683b.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/3ad8bf9.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/bad5f8d.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/c6df1bd.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/9ae19fa.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/57c422a.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e6ea745.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/60472ff.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Node.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/apple-touch-icon.png\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/favicon-32x32.png\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/favicon-16x16.png\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/favicons/safari-pinned-tab.svg\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/ionicons/css/ionicons.min.css\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/iconfont/index.css\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/ionicons/iconfont.css\n",
      "\n",
      "https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web//static/bytedesign.min.css\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e506a9f.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/176683b.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/3ad8bf9.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/bad5f8d.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/c6df1bd.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/9ae19fa.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/57c422a.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e6ea745.js\n",
      "\n",
      "//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/60472ff.js\n",
      "\n",
      "/tag/Node.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Node.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Node.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/three.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Node.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Node.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/React.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Angular.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n",
      "/tag/Vue.js\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Engine.url_parser import is_static_url\n",
    "\n",
    "with open('./csdn_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    csdn_urls = f.readlines()\n",
    "with open('./juejin_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    juejin_urls = f.readlines()\n",
    "with open('./sg_urls.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    sg_urls = f.readlines()\n",
    "urls = sg_urls + csdn_urls + juejin_urls\n",
    "print(len(urls))\n",
    "for i in range(8000):\n",
    "    if is_static_url(urls[i]):\n",
    "        print(urls[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试gerapy_auto_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/My_app/code/咻Search/Engine\")\n",
    "from gerapy_auto_extractor.classifiers.list import is_list\n",
    "from gerapy_auto_extractor.classifiers.detail import is_detail\n",
    "\n",
    "with open(\"./test.html\", 'r', encoding='utf-8') as f:\n",
    "    html= f.read()\n",
    "# 这个两个可以用\n",
    "print(is_detail(html,threshold=0.3))\n",
    "print(is_list(html,threshold=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:22:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\My_app\\Anaconda3\\envs\\XiuSearch\\lib\\site-packages\\dateparser\\date_parser.py:37: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/My_app/code/咻Search/Engine\")\n",
    "from gerapy_auto_extractor.extractors.content import extract_content\n",
    "from gerapy_auto_extractor.extractors.datetime import extract_datetime\n",
    "from gerapy_auto_extractor.extractors.list import extract_list\n",
    "from gerapy_auto_extractor.extractors.title import extract_title\n",
    "\n",
    "with open(\"./test.html\", 'r', encoding='utf-8') as f:\n",
    "    html= f.read()\n",
    "    \n",
    "# print(extract_title(html))\n",
    "# print(extract_list(html))\n",
    "print(extract_datetime(html))\n",
    "\n",
    "# 这个效果极其垃圾\n",
    "# content_html = extract_content(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noew'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"hah\":\"hello\",\"D\": 123}.get(\"hah\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zhenghui\n"
     ]
    }
   ],
   "source": [
    "print(\"zhenghui\" or \"郑辉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"hah\":\"hello\",\"D\": 123}.get(\"hah\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试查出mysql数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from config import MYSQL_HOST, MYSQL_DBNAME, MYSQL_USER, MYSQL_PASSWORD\n",
    "\n",
    "# 连接数据库\n",
    "# 加上charset='utf8'，避免 'latin-1' encoding 报错等问题\n",
    "conn = pymysql.connect(host=MYSQL_HOST, user=MYSQL_USER, passwd=MYSQL_PASSWORD, \n",
    "                       db=MYSQL_DBNAME, charset='utf8')\n",
    "\n",
    "# 创建cursor\n",
    "cursor_blogs = conn.cursor()\n",
    "cursor_list = conn.cursor()\n",
    "sql_blogs = 'SELECT page_url, urls FROM search_blogs;'\n",
    "sql_list = 'SELECT page_url, urls FROM search_blogs;'\n",
    "\n",
    "# 执行sql语句\n",
    "cursor_blogs.execute(sql_blogs)\n",
    "cursor_list.execute(sql_list)\n",
    "# 获取数据库列表信息\n",
    "col_blogs = cursor_blogs.description\n",
    "col_list = cursor_list.description\n",
    "\n",
    "# 获取全部查询信息\n",
    "re_blogs = cursor_blogs.fetchall()\n",
    "re_list = cursor_list.fetchall()\n",
    "# 获取一行信息\n",
    "# re = cursor.fetchone()\n",
    "\n",
    "# 获取的信息默认为tuple类型，将columns转换成DataFrame类型\n",
    "columns_blogs = pd.DataFrame(list(col_blogs))\n",
    "# 将数据转换成DataFrame类型，并匹配columns\n",
    "df_blogs = pd.DataFrame(list(re_blogs), columns=columns_blogs[0])\n",
    "\n",
    "columns_list = pd.DataFrame(list(col_blogs))\n",
    "# 将数据转换成DataFrame类型，并匹配columns\n",
    "df_list = pd.DataFrame(list(re_blogs), columns=columns_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ask.csdn.net/questions/363283'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "blogs_index = [url[0] for  url in re_blogs]\n",
    "list_index = [ast.literal_eval(url[1]) for  url in re_blogs]\n",
    "list_index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def urls2G():\n",
    "    '''\n",
    "    将数据库中urls的关系转化为图\n",
    "    '''\n",
    "    # 连接数据库\n",
    "    # 加上charset='utf8'，避免 'latin-1' encoding 报错等问题\n",
    "    conn = pymysql.connect(host=MYSQL_HOST, user=MYSQL_USER, passwd=MYSQL_PASSWORD, \n",
    "                        db=MYSQL_DBNAME, charset='utf8')\n",
    "    # 创建cursor\n",
    "    cursor_blogs = conn.cursor()\n",
    "    cursor_list = conn.cursor()\n",
    "    sql_blogs = 'SELECT page_url, urls FROM search_blogs;'\n",
    "    sql_list = 'SELECT page_url, urls FROM search_blogs;'\n",
    "    # 执行sql语句\n",
    "    cursor_blogs.execute(sql_blogs)\n",
    "    cursor_list.execute(sql_list)\n",
    "    # 获取全部查询信息\n",
    "    re_blogs = cursor_blogs.fetchall()\n",
    "    re_list = cursor_list.fetchall()\n",
    "    \n",
    "    # 将获取的元组信息转换为图\n",
    "    blogs_index = [url[0] for  url in re_blogs]\n",
    "    blogs_point = [ast.literal_eval(url[1]) for  url in re_blogs]\n",
    "    \n",
    "    list_index = [url[0] for  url in re_list]\n",
    "    list_point = [ast.literal_eval(url[1]) for  url in re_list]\n",
    "    indexs = blogs_index + list_index\n",
    "    points = blogs_point + list_point\n",
    "    G = np.zeros((len(indexs), len(indexs)))\n",
    "    for i, index in enumerate(indexs):\n",
    "        # 依次判断包含的url是是否在爬取过的列表中，有些广告之类的链接页会包含，但没爬取\n",
    "        for p_url in points[i]:\n",
    "            try:\n",
    "                p_index = indexs.index(p_url)\n",
    "            except:\n",
    "                p_index = -1\n",
    "            if p_index != -1:\n",
    "                G[i][p_index] = 1\n",
    "                \n",
    "    return G\n",
    "urls2G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483e81938f0d6022abef9dd89bf9a734254e81cfb1939fea0aab950c80bc2f57"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
